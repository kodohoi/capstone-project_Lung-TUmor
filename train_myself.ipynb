{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e549220c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\\\anaconda3\\lib\\site-packages\\pkg_resources\\__init__.py:122: PkgResourcesDeprecationWarning: 4.0.0-unsupported is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torchio as tio\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "logging.getLogger().handlers.clear()\n",
    "\n",
    "from unet import UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "663e470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_img_to_label_path(path):\n",
    "    parts = list(path.parts)\n",
    "    parts[parts.index(\"imagesTr\")] = \"labelsTr\"\n",
    "    return Path(*parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c430de42",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"./Task06_Lung/imagesTr/\")\n",
    "subjects_paths = list(path.glob(\"Lung*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37ee9bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = []\n",
    "\n",
    "for subject_path in subjects_paths:\n",
    "    label_path = change_img_to_label_path(subject_path)\n",
    "    subject = tio.Subject(CT = tio.ScalarImage(subject_path),\n",
    "                          Label = tio.LabelMap(label_path))\n",
    "    subjects.append(subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93255e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in subjects:\n",
    "    assert subject[\"CT\"].orientation == (\"L\", \"A\", \"S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f5ea11a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 512, 304])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjects[0][\"Label\"][\"data\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e23913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = tio.Compose([\n",
    "    tio.ToCanonical(),\n",
    "    tio.CropOrPad((500, 500, 300)),\n",
    "    tio.RescaleIntensity((-1,1))\n",
    "])\n",
    "\n",
    "augmentation = tio.RandomAffine(scales=(0.9, 1.1), degrees=(-10, 10))\n",
    "\n",
    "train_transformation = tio.Compose([preprocess, augmentation])\n",
    "val_transformation = preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f182d158",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tio.SubjectsDataset(subjects[:53], transform=train_transformation)\n",
    "val_dataset = tio.SubjectsDataset(subjects[53:], transform=val_transformation)\n",
    "\n",
    "sampler = tio.data.LabelSampler(patch_size=96, label_name=\"Label\", label_probabilities={0:0.00001, 1:0.99999})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79e50d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_patches_queue = tio.Queue(\n",
    "        train_dataset,\n",
    "        max_length=40,\n",
    "        samples_per_volume=5,\n",
    "        sampler=sampler,\n",
    "        num_workers=8)\n",
    "\n",
    "val_patches_queue = tio.Queue(\n",
    "        val_dataset,\n",
    "        max_length=40,\n",
    "        samples_per_volume=5,\n",
    "        sampler=sampler,\n",
    "        num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4c0df7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "train_loader = torch.utils.data.DataLoader(train_patches_queue, batch_size=batch_size, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_patches_queue, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9032a018",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Helper Class which implements the intermediate Convolutions\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.step = torch.nn.Sequential(torch.nn.Conv3d(in_channels, out_channels, 3, padding=1),\n",
    "                                        torch.nn.ReLU(),\n",
    "                                        torch.nn.Conv3d(out_channels, out_channels, 3, padding=1),\n",
    "                                        torch.nn.ReLU())\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.step(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "589d9ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements a UNet for the Segmentation\n",
    "    We use 3 down- and 3 UpConvolutions and two Convolutions in each step\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Sets up the U-Net Structure\n",
    "        \"\"\"\n",
    "        super().__init__()\n",

    "        self.layer1 = DoubleConv(1, 32)\n",
    "        self.layer2 = DoubleConv(32, 64)\n",
    "        self.layer3 = DoubleConv(64, 128)\n",
    "        self.layer4 = DoubleConv(128, 256)\n",

    "        self.layer5 = DoubleConv(256 + 128, 128)\n",
    "        self.layer6 = DoubleConv(128+64, 64)\n",
    "        self.layer7 = DoubleConv(64+32, 32)\n",
    "        self.layer8 = torch.nn.Conv3d(32, 1, 1)  
    "\n",
    "        self.maxpool = torch.nn.MaxPool3d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.layer1(x)\n",
    "        x1m = self.maxpool(x1)\n",
    "        x2 = self.layer2(x1m)\n",
    "        x2m = self.maxpool(x2)\n",
    "        x3 = self.layer3(x2m)\n",
    "        x3m = self.maxpool(x3)\n",
    "        x4 = self.layer4(x3m)\n",

    "        x5 = torch.nn.Upsample(scale_factor=2, mode=\"trilinear\")(x4)  # Upsample with a factor of 2\n",
    "        x5 = torch.cat([x5, x3], dim=1)  # Skip-Connection\n",
    "        x5 = self.layer5(x5)\n",
    "        x6 = torch.nn.Upsample(scale_factor=2, mode=\"trilinear\")(x5)        \n",
    "        x6 = torch.cat([x6, x2], dim=1)  # Skip-Connection    \n",
    "        x6 = self.layer6(x6)\n",
    "        x7 = torch.nn.Upsample(scale_factor=2, mode=\"trilinear\")(x6)\n",
    "        x7 = torch.cat([x7, x1], dim=1)       \n",
    "        x7 = self.layer7(x7)\n",
    "        ret = self.layer8(x7)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcc90002",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(torch.nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, pred, mask, smooth=1):\n",
    "        #flatten label and prediction tensors\n",
    "        pred = pred.view(-1)\n",
    "        mask = mask.view(-1)\n",
    "        \n",
    "        intersection = (pred * mask).sum()                            \n",
    "        dice = (2.*intersection + smooth)/(pred.sum() + mask.sum() + smooth)  \n",
    "        \n",
    "        return 1 - dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7b95c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segmenter(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = UNet()\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr = 1e-5)\n",
    "        self.loss_fn = DiceLoss()\n",
    "        \n",
    "    def forward(self, data):\n",
    "        return self.model(data)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        img = batch[\"CT\"][\"data\"]\n",
    "        label = batch[\"Label\"][\"data\"]\n",
    "        label = label.long()\n",
    "        \n",
    "        pred = self(img)\n",
    "        \n",
    "        loss = self.loss_fn(pred, label)\n",
    "        \n",
    "        self.log(\"Train Dice\", loss)\n",
    "        if batch_idx % 25 == 0:\n",
    "            self.log_images(img.cpu(), pred.cpu(), label.cpu(), \"Train\")\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        img = batch[\"CT\"][\"data\"]\n",
    "        label = batch[\"Label\"][\"data\"]\n",
    "        label = label.long()\n",
    "        \n",
    "        pred = self(img)\n",
    "        \n",
    "        loss = self.loss_fn(pred, label)\n",
    "        \n",
    "        self.log(\"Val Dice\", loss)\n",
    "        self.log_images(img.cpu(), pred.cpu(), label.cpu(), \"Val\")\n",
    "        return loss\n",
    "    \n",
    "    def log_images(self, img, pred, label, name):\n",
    "        pred = pred >0.5\n",
    "        axial_slice = 50\n",
    "        fig, axis = plt.subplots(1,2)\n",
    "        axis[0].imshow(img[0][0][:,:,axial_slice], cmap=\"bone\")\n",
    "        label_ = np.ma.masked_where(label[0][0][:,:,axial_slice]==0, label[0][0][:,:,axial_slice])\n",
    "        axis[0].imshow(label_, alpha=0.5)\n",
    "        \n",
    "        axis[1].imshow(img[0][0][:,:,axial_slice], cmap=\"bone\")\n",
    "        label_ = np.ma.masked_where(label[0][0][:,:,axial_slice]==0, label[0][0][:,:,axial_slice])\n",
    "        axis[1].imshow(label_, alpha=0.5)\n",
    "        \n",
    "        self.logger.experiment.add_figure(name, fig, self.global_step)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return [self.optimizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b20a3b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Segmenter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f766496",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"Val Dice\",\n",
    "    save_top_k=1,\n",
    "    mode=\"min\")\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor=\"Val Dice\",\n",
    "    mode=\"min\",\n",
    "    patience=20,\n",
    "    min_delta=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da9b05ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\***\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:474: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "gpus=1\n",
    "trainer = pl.Trainer(gpus=gpus, logger=TensorBoardLogger(save_dir=\"./logs\"), log_every_n_steps=1,\n",
    "                     callbacks=[checkpoint_callback, early_stop], max_epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a2f3c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name    | Type     | Params\n",
      "-------------------------------------\n",
      "0 | model   | UNet     | 5.8 M \n",
      "1 | loss_fn | DiceLoss | 0     \n",
      "-------------------------------------\n",
      "5.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.8 M     Total params\n",
      "23.344    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\***\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\***\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\data.py:84: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 10. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "C:\\Users\\***\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0108b69c2a084f98bd51a670cb9c265e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\***\\anaconda3\\lib\\site-packages\\matplotlib\\image.py:443: UserWarning: Warning: converting a masked element to nan.\n",
      "  dv = np.float64(self.norm.vmax) - np.float64(self.norm.vmin)\n",
      "C:\\Users\\***\\anaconda3\\lib\\site-packages\\matplotlib\\image.py:444: UserWarning: Warning: converting a masked element to nan.\n",
      "  vmid = np.float64(self.norm.vmin) + dv / 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1fe37df0f3644ffb7c420b5da20359b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae182a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from celluloid import Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09308a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Segmenter.load_from_checkpoint(r\"./logs/lightning_logs/version_9/checkpoints/epoch=10-step=374.ckpt\")\n",
    "model2 = model2.eval()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model2.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13c7a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = 2\n",
    "mask = val_dataset[IDX][\"Label\"][\"data\"]\n",
    "imgs = val_dataset[IDX][\"CT\"][\"data\"]\n",
    "\n",
    "# GridSampler\n",
    "grid_sampler = tio.inference.GridSampler(val_dataset[IDX], 96, (8, 8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76fc517",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregator = tio.inference.GridAggregator(grid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a4d3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_loader = torch.utils.data.DataLoader(grid_sampler, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2bfa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for patches_batch in patch_loader:\n",
    "        input_tensor = patches_batch['CT'][\"data\"].to(device)  # Get batch of patches\n",
    "        locations = patches_batch[tio.LOCATION]  # Get locations of patches\n",
    "        pred = model2(input_tensor)  # Compute prediction\n",
    "        aggregator.add_batch(pred, locations)  # Combine predictions to volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5ca2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tensor = aggregator.get_output_tensor()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabdc943",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "camera = Camera(fig)  # create the camera object from celluloid\n",
    "pred = output_tensor.argmax(0)\n",
    "\n",
    "for i in range(0, output_tensor.shape[3], 2):  # axial view\n",
    "    plt.imshow(imgs[0,:,:,i], cmap=\"bone\")\n",
    "    #mask_ = np.ma.masked_where(pred[:,:,i]==0, pred[:,:,i])\n",
    "    label_mask = np.ma.masked_where(mask[0,:,:,i]==0, mask[0,:,:,i])\n",
    "    plt.imshow(pred[:,:,i], alpha=0.5, cmap=\"autumn\")\n",
    "    #plt.imshow(label_mask, alpha=0.5, cmap=\"jet\")  # Uncomment if you want to see the label\n",
    "\n",
    "    # plt.axis(\"off\")\n",
    "    camera.snap()  # Store the current slice\n",
    "animation = camera.animate()  # create the animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb7ea9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(animation.to_html5_video()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd830404",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
